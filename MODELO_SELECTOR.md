# ü§ñ Selector de Modelos de IA

## Descripci√≥n General

El chatbot ahora soporta **dos backends de IA diferentes**:

1. **n8n (Gemini)** - Modelo predeterminado
   - Usa Google Gemini como LLM
   - Integrado con n8n workflow
   - RAG con Qdrant vector store
   - Memoria de conversaci√≥n con Redis

2. **Python RAG** - Backend personalizado
   - Backend Python con FastAPI
   - RAG con Qdrant y LangChain
   - Procesamiento personalizado de documentos

## üéØ Caracter√≠sticas

### ‚úÖ Selector Visual
- Dropdown en la interfaz de chat
- √çconos distintivos: ü§ñ para n8n, üêç para Python
- Descripci√≥n de cada modelo

### ‚úÖ Modal de Confirmaci√≥n
- Alerta al usuario cuando intenta cambiar de modelo
- Explica que se crear√° una nueva conversaci√≥n
- Muestra caracter√≠sticas del modelo seleccionado
- Opciones de Confirmar o Cancelar

### ‚úÖ Gesti√≥n de Conversaciones
- Cada conversaci√≥n guarda el modelo utilizado
- Al cambiar de modelo se crea autom√°ticamente una nueva conversaci√≥n
- La conversaci√≥n actual se guarda antes del cambio

### ‚úÖ Persistencia
- El modelo se guarda en `conversation.settings` (campo JSON)
- Las conversaciones mantienen su modelo original

## üìÅ Archivos Creados/Modificados

### Nuevos Componentes

1. **`components/ModelSelector.tsx`**
   - Componente dropdown para seleccionar modelo
   - Tipo: `AIModel = 'n8n' | 'python'`
   - Props: `currentModel`, `onModelChange`, `disabled`

2. **`components/ModelChangeModal.tsx`**
   - Modal de confirmaci√≥n para cambio de modelo
   - Muestra modelo origen y destino
   - Lista caracter√≠sticas del nuevo modelo
   - Botones: Cancelar / Crear Nueva Conversaci√≥n

3. **`app/api/chat/python/route.ts`**
   - Endpoint para el backend Python
   - Proxy a `http://localhost:8000/chat`
   - Maneja streaming SSE
   - Guarda mensajes en PostgreSQL

### Archivos Modificados

1. **`hooks/useChat.ts`**
   - Ahora acepta par√°metro `model: 'n8n' | 'python'`
   - Enruta a `/api/chat/send` (n8n) o `/api/chat/python` (Python)
   - Pasa modelo en settings de la petici√≥n

2. **`components/ChatInput.tsx`**
   - Integra `ModelSelector`
   - Props nuevas: `currentModel`, `onModelChange`
   - Pasa modelo actual al enviar mensajes

3. **`app/[locale]/chat/page.tsx`**
   - Estado para modelo actual (`currentModel`)
   - Estado para modelo pendiente (`pendingModel`)
   - Control del modal de cambio (`showModelChangeModal`)
   - L√≥gica para confirmar/cancelar cambio de modelo
   - Reset de conversaci√≥n al cambiar modelo

4. **`.env.example`**
   - Nueva variable: `PYTHON_BACKEND_URL`

## üöÄ Configuraci√≥n

### 1. Variables de Entorno

Crea un archivo `.env.local` (si no existe) y agrega:

```env
# Python Backend
PYTHON_BACKEND_URL="http://localhost:8000"
```

### 2. Levantar Backend Python

Navega a la carpeta del backend Python:

```bash
cd python/backend
```

Instala dependencias (si no lo has hecho):

```bash
pip install -r requirements.txt
```

Levanta el servidor:

```bash
python app.py
```

El servidor deber√≠a estar corriendo en `http://localhost:8000`

### 3. Verificar Qdrant

Aseg√∫rate de que Qdrant est√© corriendo:

```bash
# Si usas Docker:
docker run -p 6333:6333 qdrant/qdrant
```

## üìñ Uso

### Para Usuarios

1. **Seleccionar Modelo**
   - En la interfaz de chat, haz clic en el dropdown del selector de modelo
   - Ver√°s las opciones: n8n (Gemini) y Python RAG

2. **Cambiar de Modelo**
   - Selecciona el modelo deseado
   - Si tienes una conversaci√≥n activa, aparecer√° un modal de confirmaci√≥n
   - El modal te informa que se crear√° una nueva conversaci√≥n
   - Confirma el cambio o canc√©lalo

3. **Nueva Conversaci√≥n**
   - Al confirmar, la conversaci√≥n actual se guarda autom√°ticamente
   - Se crea una nueva conversaci√≥n con el modelo seleccionado
   - Puedes empezar a chatear inmediatamente

4. **Conversaciones Guardadas**
   - Cada conversaci√≥n recuerda qu√© modelo utiliz√≥
   - Al abrir una conversaci√≥n antigua, se mantiene su modelo original

### Para Desarrolladores

#### Tipo `AIModel`

```typescript
export type AIModel = 'n8n' | 'python';
```

#### Uso del Hook `useChat`

```typescript
const { handleSubmit } = useChat();

// En el submit:
handleSubmit(e, 'n8n');      // Usa n8n
handleSubmit(e, 'python');   // Usa Python
```

#### Estructura de la Petici√≥n

```typescript
// Petici√≥n a /api/chat/send (n8n) o /api/chat/python
{
  message: "Hola, ¬øc√≥mo est√°s?",
  conversationId: "uuid-conversation",
  settings: {
    topK: 5,
    temperature: 0.7,
    model: "n8n" // o "python"
  }
}
```

#### Respuesta SSE (Server-Sent Events)

Ambos endpoints devuelven eventos SSE con este formato:

```
data: {"type":"message","data":{"content":"Hola "}}
data: {"type":"message","data":{"content":"mundo"}}
data: {"type":"sources","data":{"sources":[...]}}
data: {"type":"complete","data":{"conversationId":"uuid"}}
```

## üîß Personalizaci√≥n

### Agregar Nuevo Modelo

1. **Actualizar tipo `AIModel`** en `components/ModelSelector.tsx`:

```typescript
export type AIModel = 'n8n' | 'python' | 'nuevo-modelo';
```

2. **Agregar al array `models`**:

```typescript
const models = [
  { id: 'n8n', name: 'n8n (Gemini)', description: '...', icon: 'ü§ñ' },
  { id: 'python', name: 'Python RAG', description: '...', icon: 'üêç' },
  { id: 'nuevo-modelo', name: 'Nuevo', description: '...', icon: 'üéØ' }
];
```

3. **Crear endpoint** en `app/api/chat/nuevo-modelo/route.ts`

4. **Actualizar routing** en `useChat.ts`:

```typescript
const endpoint = 
  model === 'python' ? '/api/chat/python' :
  model === 'nuevo-modelo' ? '/api/chat/nuevo-modelo' :
  '/api/chat/send';
```

### Personalizar Modal

Edita `components/ModelChangeModal.tsx` para:
- Cambiar colores
- Modificar textos
- Agregar m√°s informaci√≥n
- Cambiar animaciones

### Cambiar Modelo por Defecto

En `app/[locale]/chat/page.tsx`:

```typescript
const [currentModel, setCurrentModel] = useState<AIModel>('python'); // Cambiar aqu√≠
```

## üß™ Testing

### Flujo Completo

1. Abre el chat
2. Env√≠a un mensaje con modelo n8n (default)
3. Intenta cambiar a Python
4. Verifica que aparece el modal
5. Confirma el cambio
6. Verifica que se crea nueva conversaci√≥n
7. Env√≠a mensaje con Python
8. Revisa el sidebar - deber√≠as ver dos conversaciones

### Verificar Endpoints

```bash
# n8n endpoint (deber√≠a estar funcionando)
curl -X POST http://localhost:3000/api/chat/send \
  -H "Content-Type: application/json" \
  -d '{"message":"Hola","conversationId":"test","settings":{"model":"n8n"}}'

# Python endpoint (requiere backend Python corriendo)
curl -X POST http://localhost:3000/api/chat/python \
  -H "Content-Type: application/json" \
  -d '{"message":"Hola","conversationId":"test","settings":{"model":"python"}}'
```

## üêõ Soluci√≥n de Problemas

### Error: "Python backend error: 500"

**Causa**: El backend Python no est√° corriendo o no est√° accesible.

**Soluci√≥n**:
1. Verifica que Python backend est√© corriendo: `http://localhost:8000`
2. Revisa la variable `PYTHON_BACKEND_URL` en `.env.local`
3. Verifica logs del backend Python

### Modal no aparece al cambiar modelo

**Causa**: No hay conversaci√≥n activa.

**Comportamiento esperado**: Si no hay mensajes, el modelo cambia directamente sin modal.

### Conversaci√≥n no se guarda con el modelo correcto

**Causa**: El campo `settings` no est√° configurado.

**Soluci√≥n**: Verifica que el endpoint guarde:

```typescript
await prisma.conversation.create({
  data: {
    // ...
    settings: {
      model: 'python', // Importante!
      ...settings
    }
  }
});
```

### Backend Python no recibe mensajes

**Causa**: URL incorrecta o formato de petici√≥n incompatible.

**Soluci√≥n**:
1. Verifica que el backend Python est√© en `http://localhost:8000`
2. Revisa que el endpoint sea `/chat`
3. Verifica el formato JSON esperado por Python

## üìä Diagrama de Flujo

```
Usuario selecciona modelo
        ‚Üì
¬øHay conversaci√≥n activa?
   ‚Üì                    ‚Üì
  S√ç                   NO
   ‚Üì                    ‚Üì
Mostrar modal      Cambiar directamente
   ‚Üì
Usuario confirma/cancela
   ‚Üì
Si confirma:
  - Guardar conversaci√≥n actual
  - Resetear chat
  - Cambiar modelo
  - Nueva conversaci√≥n
```

## üé® Personalizaci√≥n Visual

### Colores de Modelos

En `ModelSelector.tsx`, cada modelo tiene un color:

```typescript
// n8n: azul
className="text-blue-600"

// Python: verde
className="text-green-600"
```

Puedes cambiarlos editando las clases de Tailwind.

### Iconos

Puedes cambiar los emojis o usar iconos SVG:

```typescript
const models = [
  { id: 'n8n', icon: 'ü§ñ' },  // Cambiar aqu√≠
  { id: 'python', icon: 'üêç' }  // Cambiar aqu√≠
];
```

## üìù Notas Importantes

1. **Default es n8n**: El modelo predeterminado es n8n (Gemini)
2. **Conversaciones separadas**: Cada modelo mantiene conversaciones independientes
3. **Persistencia**: El modelo se guarda en `conversation.settings` (JSON)
4. **Streaming**: Ambos endpoints soportan SSE para respuestas en tiempo real
5. **Autenticaci√≥n**: Las conversaciones se guardan en PostgreSQL para usuarios autenticados, en localStorage para invitados

## üîÆ Futuras Mejoras

- [ ] Permitir cambiar modelo dentro de una conversaci√≥n (sin crear nueva)
- [ ] Mostrar indicador del modelo actual en el header
- [ ] Estad√≠sticas de uso por modelo
- [ ] Configuraci√≥n personalizada por modelo (temperatura, topK, etc.)
- [ ] Soporte para m√°s backends (Claude, GPT-4, etc.)
- [ ] Comparaci√≥n lado a lado de respuestas de diferentes modelos
